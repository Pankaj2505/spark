{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1baabcf3",
   "metadata": {},
   "source": [
    "# History of  SPARK\n",
    "\n",
    "#### AS data is growing because of IOT and internet companies. so challanges related to data.\n",
    "- Data collection and Ingestion \n",
    "- data storage and management\n",
    "- Data processing and transformation\n",
    "- Data Access and Retrieval\n",
    "\n",
    "#### Solution to challange\n",
    "- Hadoop has implemented GFs(Google file system ) with HDFS  to store large data into distributed system\n",
    "    - hdfs is highliy scalable , you can add as many cluster to it as data size grows\n",
    "- Map Reduce solved the compute capacity, by implementing a distributed computation framework over hdfs.\n",
    "    - it break the computing process to smaller task,\n",
    "    - use cluster of computer to process individual task \n",
    "    - lastly combine the outcome of each task to produce consolidated result\n",
    "\n",
    "- Other projects based on google file system and google map reduce similarly like hadoop created a mapping system, which will convert user friendly code to map reduce code.\n",
    "    - Hadoop(opensource) data efficient and not compute efficient as processing data used to stored in hard disk.\n",
    "    - pig (yahoo) similar to sql\n",
    "    - hive(facebook) closer to sql\n",
    "    - apache spark(open source) closer to programming language like python and jave\n",
    "        - apache spark has many API's to work with hadoop filesystm and mapreduce \n",
    "        - spark sql\n",
    "        - spark streaming\n",
    "        - MLlib(Machine learning)\n",
    "        - graphX (Graph API)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14403546",
   "metadata": {},
   "source": [
    "# Background to Distributed computing\n",
    "\n",
    "#### Hadoop vs Datawarehouse\n",
    "- Before Hadoop how do we handle large data ?\n",
    "    - Data warehouses like tera data and exa data\n",
    "    - we create pipelines to gets data from oltp system and brought them in datawarehouse\n",
    "\n",
    "- why big data is better than DW\n",
    "    - Horizontal Scalability \n",
    "        - simply by adding hardware we can grow the system\n",
    "    - low Capital cost\n",
    "        - we can add low cost storage cluster \n",
    "        - we can add low cost srevers for processing\n",
    "        \n",
    "#### Hadoop (onprimise solution) vs cloud \n",
    "- Datalake is the cloud version of hadoop\n",
    "- data lake has below capabilities \n",
    "    - ingestion : data capture and ingestion(integration)\n",
    "    - store : data storage and management\n",
    "    - process : data process and transformation\n",
    "    - consume : data access and retrieval\n",
    "- in data lake is a repository where we keep raw/unprocessed data,  and sometimes processed data also \n",
    "- marketplayer \n",
    "    - ingestion : data capture and ingestion(integration)\n",
    "        - tool which brings data from source system to Datalake\n",
    "            - informatica\n",
    "            - talend\n",
    "            - DW using (pipeline)\n",
    "        \n",
    "        - properties\n",
    "            - its a raw data\n",
    "            - which we will not change\n",
    "    \n",
    "    - store : data storage and management\n",
    "        - hadoop(onprimises)\n",
    "        - amazon s3\n",
    "        - azure blob\n",
    "        - google cloud\n",
    "      \n",
    "        - properties\n",
    "            - highly scalable\n",
    "            - highly available\n",
    "            - low cost\n",
    "    \n",
    "    - process : data process and transformation\n",
    "      - all computaion happens\n",
    "          - intital data quality check \n",
    "          - transformation and preperation of data\n",
    "          - corelating\n",
    "          - aggegating\n",
    "          - Analyzing (extraction of busniness insight)\n",
    "          - Applying machine learning model\n",
    "      - to do computin at large scale , process layer broke into two part\n",
    "          - data processing framwork\n",
    "              - it allows us to design and develop distributed compuating application\n",
    "              - ex- apache spark\n",
    "          - orchestration\n",
    "              - it is responsible for the formation of cluster\n",
    "              - managing resources\n",
    "                  - scaling up\n",
    "                  - scaling down\n",
    "              - ex: kunernetes, hadoop yarn, apache mesos\n",
    "              \n",
    "    - consume : data access and retrieval\n",
    "        - putting data for real life process\n",
    "            - data scitist might want to access dat from lake\n",
    "            - application and dashboard\n",
    "            - jdbc /odbc framework\n",
    "            - rest api dashborad\n",
    "- chanllanges with data lake\n",
    "    - security and access control\n",
    "    - scheduling and workflow mangement\n",
    "    - data catalog and meta data\n",
    "    - data life cycle and governance\n",
    "    - operation and monitoring\n",
    "            \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f67414",
   "metadata": {},
   "source": [
    "# Spark Archietecture\n",
    "#### What\n",
    "   - Spark Ecosystem has two layer\n",
    "       - Spark next top most layer (Set of DSL, Libraries, API)\n",
    "           - spark sql framework\n",
    "           - streaming\n",
    "           - ML lib\n",
    "           - Graph X\n",
    "       - Spark core (Runs on cluster of server to provide distributed system)\n",
    "           - Spark Core API's\n",
    "               - Scala\n",
    "               - Java\n",
    "               - Python \n",
    "               - R\n",
    "           - Distributed computing engine(Spark Engine)\n",
    "              \n",
    "           - Cluster Manager\n",
    "                - __Spark does not manage cluster, it just give you a framework to work with HDFS and MAPreduce embedded cluster so we need a cluster manager__\n",
    "               - Resource manger or content orchastrator\n",
    "               - commonly we use hadoop YARN, as Spark runs on Hadoop paltform\n",
    "               - we have kubernetes and apache mesos\n",
    "           - Distributed Storage\n",
    "               - __Spark does not come with inbuilt storage system. it allows to process data which is stored in variety of storage__\n",
    "               - commonly use storage HDFS, blob, S3, google cloud storage, casandra system\n",
    "   \n",
    "#### More on SPARK architecture   \n",
    "- __Apache Spark does not provide storage and  cluster managment__\n",
    "- spark compute engine manage the processing part\n",
    "    - it breaks the process into smallar task\n",
    "    - Scheduling those task to cluster for parallel processing\n",
    "    - Providing data to these task\n",
    "    - managing and monitoring the task\n",
    "    - provide intolrance in case of process failure\n",
    "    - interacting with storage manager and cluster manager\n",
    "    - cosolidate the output of each task \n",
    "- Spark Core API's\n",
    "    - its a programming interface layer which offers core API's in 4 language\n",
    "        - Scala\n",
    "        - python \n",
    "        - java\n",
    "        - R\n",
    "    - using these API's we used to write data processing logic but these API's are little tricky\n",
    "        - they are hard to learn as new syntax for common work\n",
    "        - lacks performance optimization features\n",
    "- Spark External API's\n",
    "    - Set of API'S, Packages, Libraries\n",
    "    - developed over Spark core APIs, that mean internally these API's will use Spark CORE API's\n",
    "    - Spark Libraries\n",
    "        - Spark SQL\n",
    "        - Spark Data frame and data set (Fucntional programming technique)(data crunching )\n",
    "        - Streaming(allow process countinous Data \n",
    "        - MLlib( to process ML and DL problem\n",
    "        - GraphX\n",
    "        \n",
    "####  Why Spark \n",
    "- Abstraction\n",
    "    - abstarct the fact that you are coding on a program to execute on cluster of computer , it gives a feel that we are working on database, python dataframe\n",
    "    - all complixity like distributed storage, parallel processing, computation is abstracted away\n",
    "- Unified Platform\n",
    "    - here we can do batch processing , stream processing, structured and semi structured data processing, graph processing, ML &DL code processing at single platform\n",
    "- Ease of use \n",
    "    - use ready to use libraries,Alogorithm, and integration with other tool\n",
    "    - code are shorter , simple\n",
    "        \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2b561d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
